<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 7.6 开发与运维

<center> <img src="./img/5/../6/7-6-5-devopsplatform.png" ch="500" width="700" height="300" /></center>
<center>图7-6-0. 平台DevOps </center>

在之前的章节，我们已经介绍面向深度学习的集群管理系统的运行时，调度与存储。如图7-6-0所示，本章将围绕工程实践中，关于集群管理系统功能开发，运维的主题进行展开。集群管理系统，也称作平台，是$ 7 \times 24 $小时的对公司内部使用或者对外提供服务的平台型服务，其本身的开发越来越敏捷，同时也需要有更加敏捷而高效的运维支持。

- [7.6 开发与运维](#76-开发与运维)
  - [7.6.1 平台功能模块与敏捷开发](#761-平台功能模块与敏捷开发)
  - [7.6.2 监控体系构建](#762-监控体系构建)
      - [***全局监控***](#全局监控)
      - [***性能监控***](#性能监控)
      - [***服务与硬件稳定性监控***](#服务与硬件稳定性监控)
      - [***报警***](#报警)
  - [7.6.3 测试](#763-测试)
    - [7.6.3.1 单元测试](#7631-单元测试)
    - [7.6.3.2 集成测试](#7632-集成测试)
    - [7.6.3.3 压力测试](#7633-压力测试)
    - [7.6.3.4 回归测试](#7634-回归测试)
    - [7.6.3.5 缺陷注入，模糊测试与混沌工程](#7635-缺陷注入模糊测试与混沌工程)
  - [7.6.4 平台DevOps](#764-平台devops)
    - [数据流](#数据流)
  - [7.6.5 平台运维](#765-平台运维)
    - [7.6.5.1 直接责任人（DRI）与候命（On-Call）机制](#7651-直接责任人dri与候命on-call机制)
    - [7.6.5.2 事件管理](#7652-事件管理)
    - [7.6.5.3 智能运维（AIOps）](#7653-智能运维aiops)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)
  
## 7.6.1 平台功能模块与敏捷开发

平台本身由于功能众多，部署后持续运行类似在线服务，业界一般通过敏捷开发的模式进行整体的开发与设计。一般平台中涉及以下重要服务和功能模块的开发与设计。

- 内核：调度器，运行时，管理界面，权限管理，文件管理等。
  - 调度器设计可以兼顾深度学习特点例如资源局部性。
  - 运行时需要支持特定设备插件，能够将GPU，IB等设备注册进入运行时。
  - 由于深度学习作业主要输入输出为数据和模型，一般可使用文件系统存储即可，需要选用高效（NFS等）且容错（云存储）的文件系统提供支持。
- 监控与报警：性能监控，异常监控，报警系统等。
  - 除常见需要平台监控的性能指标，还要特别关注GPU等核心加速器的资源利用率等指标。
  - 报警系统也要除覆盖常见错误，还需要覆盖新服务与新硬件产生的错误与异常。
- 工具链：IDE，作业提交与调试工具，API(例如：Restful)等。
  - IDE可以选用对Python和深度学习生态支持较好的开发环境。例如，基于浏览器的开发环境（例如，Jupyter）或者插件化支持较好的本地客户端环境VS Code等。
  - 作业提交可以通过Web人工提交或者Restful API方式方便AutoML工具自动化提交。
  - 可以考虑支持远程SSH方便用户远程登录作业现场调试。
- 应用市场：模型市场，镜像市场等。
  - 由于目前深度学习新的模型很多基于研究工作和开源模型与代码进行微调（Fine Tuning）适配和扩展，模型训练框架主流框架也都是开源的，这就让平台方可以通过提供常用和优化版本的模型与框架镜像市场进而优化，管理和提升开发生产力。

不同的功能模块可以分配不同的工程师进行开发，通过[Scrum](https://en.wikipedia.org/wiki/Scrum_(software_development))敏捷开发机制，定期规划与执行Sprint计划，并通过每日的Scrum进行规划反思与推进。

## 7.6.2 监控体系构建

通过构建完整的指标体系，能够让平台管理员和开发者对平台健康状况和系统负载运行状况一目了然，同时有助于对异常与错误进行诊断与归因分析。围绕异构资源管理系统，可以从以下方面设计指标， 进而通过数据驱动的方法提升运维效率。

#### ***全局监控***

可以通过围绕作业，服务，节点几个维度设计需要整体宏观监控的指标，并围绕团队的核心KPI设计效率，稳定性等全局指标，方便量化评估与回查。

<center> <img src="./img/6/7-6-1-monitorpage.jpg" ch="500" width="800" height="600" /></center>
<center>图7-6-1. 平台全局监控(<a href="https://github.com/microsoft/pai">图片来源</a>)</center>

如图所示，平台可以通过全局监控观测硬件资源的利用率，健康状况，也可以拓展到作业，用户等其他指标。通过宏观监控让管理员和运维工程师能观测重要问题，对整体有宏观认识。

#### ***性能监控***

可以围绕作业和硬件的性能进行监控设计，其中硬件的各种指标在作业执行时间的连接可以推导出作业的性能指标。对深度学习系统，尤其需要关注GPU的利用率等指标，这可以通过[NVML](https://developer.nvidia.com/nvidia-management-library-nvml)，[DCGM](https://developer.nvidia.com/dcgm)等工具构建指标导出器进行监控与获取。

如下脚本所示，我们可以通过nvidia-smi命令获取GPU的利用率，显存使用状况，温度和功耗，以及是否有ECC error等信息，进而将这些信息暴露给监控系统收集，进而形成对系统性能和健康的持续监控。

```shell
$ nvidia-smi
...
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 00004F2F:00:00.0 Off |                    0 |
| N/A   51C    P0    56W / 149W |   2513MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           On   | 00007C52:00:00.0 Off |                    0 |
| N/A   31C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

```

#### ***服务与硬件稳定性监控***

可以观测和监控平台的各个模块的健康，通过被动汇报或者主动探测的机制，监测服务的存活状况。对硬件可以周期性运行测试，或者通过监控指标的异常进行判断。服务模块在设计之初就需要考虑暴露接口或者心跳机制设计，可以被监控系统更好的遥测。

很多平台和服务提供了服务存活检测的机制，让服务开发者在服务设计之初就考虑好系统健康状况的可观测性。

例如，以Kubernetes为例，许多长时间运行的应用程序最终会转变为损坏状态，除非重新启动，否则无法恢复。Kubernetes提供了活性探针来检测和补救这种情况。

在以下Kubernetes官方提供的实例中，创建一个运行基于 k8s.gcr.io/busybox 映像的容器的Pod。在容器生命的前 30 秒，有一个 /tmp/healthy 文件。因此，在前 30 秒内，命令 cat /tmp/healthy 返回一个成功代码。30 秒后， cat /tmp/healthy由于执行失败，返回失败代码。此实例可以推广开来，用户可以替换要执行的反映状态的命令，也可以替换不同的监测状态的命令。

下面的yaml文件是 Pod 的配置文件([文件来源](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/))：

```yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    # 服务启动时执行以下命令
    args: 
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
```

#### ***报警***

监控系统可以在节点部署监控脚本(Exporter)以及存储监控指标数据的时序数据库（例如，[Promethus](https://prometheus.io/), [Ganglia](http://ganglia.sourceforge.net/)等）进行监控数据收集，并通过可视化的系统(例如，[Grafana](https://grafana.com/)等)进行监控报表和可视化的展示。
报警系统（例如，[Alert Manager](https://prometheus.io/docs/alerting/latest/alertmanager/)）可以提前规划好报警规则与阈值，不断监控和分析监控系统中的指标是否违规，如果违规及时发送报警信息或者邮件。尤其对用户提交的作业，深度学习作业容易发生OOM，GPU容易出现阻塞或者ECC错误等问题，对于高频和影响较大的异常需要及时修复与处理。

例如，我们可以设计以下的一些报警规则，并通过系统触发报警让运维人员介入：

1. 负载和性能问题规则：例如，例如磁盘使用率超过一定阈值，网络性能低于一定阈值等
2. 服务健康规则：例如，服务失败报警等
3. 硬件健康规则：例如，节点，GPU或磁盘出现故障的报警等

<center> <img src="./img/6/7-6-2-monitor.png" ch="500" width="900" height="600" /></center>
<center>图7-6-2. 监控系统(<a href="https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-packages-monitoring.html">图片来源</a>)</center>

如图所示，一个完整的监控系统如图所示，节点部署监控脚本(例如node-exporter)收集各种性能指标和健康指标，监控指标被存储到时序数据库（例如Promethus）中，报警管理系统周期性查询时序数据库判断是否触发报警，如果触发可以通知接受者（例如通过邮件），同时可视化报表系统也可以查询数据库绘制可视化仪表盘方便管理员监测当前情况。整体的各个系统组件可以通过集群管理系统（例如Kuberenetes）进行部署和运维。

在构建报警系统之初，需要决定以怎样的工具或者方式通知到相应的负责人进行响应和处理。目前有以下常见的多渠道通知的方式，他们各有利弊，一般生产环境可以根据工作时间与非工作时间，以及报警的严重程度综合配置通知软件：

- 电话报警通知：适合紧急处理的故障，电话通知最为直接。特别是在非工作时间发生的重大故障，电话是最快的通知到责任人的方式，所以在候命值班的工程师在责任日需要24小时开机待命。
- 短信报警通知：在短信中可以自行定义通知到的内容包括哪些，但是通知需要订阅短信服务，有一定的成本，不容易被内部系统记录，不容易拉群讨论。
- 即时通讯工具报警通知：报警系统也可以通过直接与IM软件绑定，方便报警转发与讨论，消息也比较及时。例如，Teams，钉钉，企业微信等协作工具，有助于创建群讨论故障，快速协同相关人进行良好协作，共同解决故障。
- 邮件报警通知：适合在工作时间进行报警通知，非工作时间容易错过消息，邮件适合事后分析，追踪与转发事故案例，方便回溯。

## 7.6.3 测试

可以通过 以下几个方面进行系统测试保证平台的常见问题，早发现，早规避，主动出击保证平台软件代码和服务本身的可靠性与质量。

### 7.6.3.1 单元测试

平台的新增功能模块需要进行单元测试，保证代码功能性正确性。可以选用所使用的模块的开发语言的单元测试库进行单元测试用例的开发。在回归测试中也可以触发单元测试进行回测。单元测试库一般根据平台开发使用的语言而确定，例如，Python（常用于构建部署脚本，监控等）的[PyUnit](https://wiki.python.org/moin/PyUnit)，Java（常用于构建服务，节点管理器等）的[JUnit](https://junit.org/junit5/)，Golang（构常用于建服务，节点管理器等）的[testing package](https://go.dev/doc/tutorial/add-a-test)，Rust（常用于构建服务，节点管理器等）的[Unit testing](https://doc.rust-lang.org/rust-by-example/testing/unit_testing.html)。目前这些语言的选择一般是根据团队背景和组件需求，在平台与服务中常常用来构建特定的组件。

### 7.6.3.2 集成测试

由于开源系统的应用越来越广泛，目前很多平台系统或多或少的模块会采用开源系统进行实现和部署，这就造成平台整体模块间的互操作较多，集成测试变得尤为重要。可以在测试集群或者环境中进行集成测试。每次有一定的功能更新在开发分支需要合并到主分支之前，可以触发集成测试，将各个模块都进行打包，并在测试环境整体部署，运行一定的测试作业或者检测脚本，验证不会产生构建版本冲突，服务调用冲突等由于模块互操作产生的问题。没有问题后再进行相应的分支的合并，为之后的测试或者上线做好准备。在每次发布都需要进行集成测试。

### 7.6.3.3 压力测试

对于平台，可以不间断的自动提交一些测试作业进行巡检，也可以定期进行压力测试检测服务功能的负载能力或硬件的稳定性。例如，[SuperBenchmark](https://github.com/microsoft/superbenchmark)支持对人工智能基础平台和硬件提供AI工作负载基准测试和分析，提供不同现有硬件之间的综合性能比较，它为原始计算和通信基准测试提供微基准，以及用于测量域感知端到端深度学习工作负载的模型基准。

平台也可以通过经典深度学习基准测试（例如，[MLPerf](https://github.com/mlperf)等）中的负载进行性能与压力测试。MLPerf 是一个由来自学术界、研究实验室和工业界的组织组成的联盟和基准测试，其使命是“建立公平和有用的基准”，为硬件、软件和服务的训练和推理性能提供公正的评估。2018 年 5 月 2 日
加州大学伯克利分校的 RISE 实验室今天与百度、谷歌、哈佛大学和斯坦福大学一起在纽约市的 O'Reilly AI 会议上发布MLPerf。截止到2019年，MLPerf 是由行业（40 多家公司）以及工程师和研究人员（1000 多名）推动的机器学习基准标准。 基准套件包括一组关键的机器学习训练和推理工作负载，它们代表了重要的生产用例，从图像分类和对象检测到推荐。

***经典回顾***

历史启示。MLPerf的部分动力来自于[通用计算的系统性能评估合作 (System Performance Evaluation Cooperative)简称(SPEC) 基准](https://www.spec.org/benchmarks.html)，该基准在 1980 年代开始的几十年里推动了快速、可衡量的性能改进。

### 7.6.3.4 回归测试

一旦完成平台新功能开发或者在线热修复，需要进行回归测试保证符合之前系统假设。
回归测试是指修改了已有代码后，重新进行测试以确认修改没有引入新的错误或导致其他代码产生错误。由于当前平台属于在线服务，同时常常使用敏捷开发模式，这样十分容易产生软件缺陷，设计好回归测试的机制，对保证平台的正确性显得尤为重要。

### 7.6.3.5 缺陷注入，模糊测试与混沌工程

同时也可以考虑使用模糊测试（Fuzzing）主动注入缺陷（Fault Injection），提前修复和增强平台的稳定性，当前工业界也习惯在平台领域的这类测试称其为混沌工程（Chaos Engineering）。常用的开源混沌工程系统一般是针对通用基础平台（例如代表性的[Chaos Monkey](https://netflix.github.io/chaosmonkey/)），对深度学习和基于Kuberenetes的平台（例如，[Chaos Mesh](https://chaos-mesh.org/)）可以加入新的插件以支持大规模领域特定异常和故障注入。

<center> <img src="./img/6/7-6-1-chaos.png" ch="500" width="1000" height="500" /></center>
<center>图7-6-3. 混沌工程通过模糊测试(Fuzzing)为系统主动注入故障(Failure)，增强系统稳定性。(<a href="">图片来源</a>)</center>

如图7-6-3.所示，针对深度学习平台，除了传统技术栈的故障依旧注入之外，还可以考虑针对GPU和Infiniband以及上层的软件栈注入领域特定的缺陷，提升缺陷覆盖度，较早且主动的发现相应的平台问题并处理与加固平台系统。

***经典回顾***

[缺陷注入（Fault Injection）与模糊测试（Fuzzing）](https://en.wikipedia.org/wiki/Fault_injection#:~:text=Fault%20injection%20is%20a%20testing,or%20using%20a%20hybrid%20approach.)：故障注入是一种软件工程中的测试技术，用于了解计算系统在受到异常压力时的行为方式。这可以使用基于物理或软件的方法或使用混合方法来实现。硬件测试驱动的故障注入包括在计算机内存和中央处理器上应用高压、极端温度和电磁脉冲，通过将组件暴露在超出其预期操作限制的条件下，计算系统可能会被迫错误执行指令并破坏关键数据。在软件测试中，故障注入是一种通过将故障引入测试代码路径来提高测试覆盖率的技术。特别是错误处理代码路径，否则可能很少遵循。它通常与压力测试一起使用，并被广泛认为是开发健壮软件的重要组成部分。模糊测试（Fuzzing）是一种故障注入，通常用于测试协议、命令行参数，API 等接口中的漏洞。

## 7.6.4 平台DevOps

可以通过CI/CD机制，持续集成（Continuous Integration），简称（CI），持续交付（Continuous Delivery）简称（CD），也就是我们通常所说的[开发运维（DevOps）](https://en.wikipedia.org/wiki/DevOps)。平台本身的各个服务组件也可以打包为Docker镜像，通过Kubernetes等工具部署和管理相应服务。可以通过相关的工具（例如：[Jenkins](https://www.jenkins.io/)），构建CI/CD的流水线，进而保证平台工程时开发新功能或者热修复(Hotfix)后能够自动化走完完整的测试与部署流程，大幅提升开发与部署效率。

<center> <img src="./img/6/7-6-4-platformCICD.png" ch="500" width="1000" height="500" /></center>
<center>图7-6-4. 深度学习平台本身开发DevOps，一个新功能Pull Request的CI/CD流程时序图。（点击查看全图）</center>

### 数据流

如图7-6-4.本实例涉及一个平台工程师开发一个新功能并上线到生成平台的 DevOps 管道。 数据流经方案的流程如下所示：

1. 开发人员更改平台系统某模块程序源代码。
2. 所做的代码更改提交到源代码管理存储库，例如 GitHub。代码管理库触发一定测试，例如单元测试等。如果不通过，则开发人员修复，并重走刚才的流程。
3. 为了启动持续集成 (CI) 过程，Webhook 会触发一个 Jenkins 项目生成。
4. Jenkins 触发一定的测试，构建镜像，并将镜像推送到镜像中心。如果不通过，则开发人员修复，并重走刚才的流程。
5. Jenkins 通过之前定义好的流程，再触发系统到测试平台进行集成测试，压力测试，混沌测试等。如果不通过，则开发人员修复，并重走刚才的流程。
6. Jenkins 通过持续部署 (CD) 将这个更新的容器映像部署到生产环境集群。如果不通过，则开发人员修复，并重走刚才的流程。
7. 终端用户不断向平台提交作业，平台运维工程师进行线上运维。如果需要热修复，则开发人员修复，并重走刚才的流程。
8. 重复1-8的过程。


## 7.6.5 平台运维

### 7.6.5.1 直接责任人（DRI）与候命（On-Call）机制

候命（On-Call）机制与之相接近的就是值班，是指平台方为了快速相应平台故障（Fault）或者重大事件（Incident），在某段时间内指定某个人（一般叫做直接责任人（Directly Responsible Individuals））或者某组人随时待命（类似值班）。在故障发生的一瞬间，通过之前的监控与报警系统捕获到问题，之后以邮件、短信、电话等形式通知到负责人，以确保第一时间的响应与处理。

当平台上线部署后，就应该建立支撑的流程和机制。如果使用的监控工具单一，那么集中化就不是最必要的，如何有序处理突发事件更为重要。随着机器越来越多，运维团队人数变多，就逐步需要设置支撑流程和响应机制了。
 - 建立多个梯度，例如，一线、二线，或者设置到第三线候命团队，一线一般为$7 \times 24$小时的候命直接责任人（Directly Responsible Individuals）。$7 \times 24$小时的值班制度不论对于责任人还是整个团队都有较高代价，团队可以通过系统制订排班，实现日、周、月的轮流机制，能让团队责任人经验共享的同时，能够保持一定的休息和工作生活平衡，更加高效的支持平台运维。
 - 二线可以是较为资深工程师，或者是平台研发工程师，他们对系统的疑难杂症更加有经验。
 - 三线可以设置为团队主管，或者汇报线上更高级别的主管。同时可以有外部的厂家，如硬件、云服务等相关服务方。

同时针对不同的告警级别，进行不同的服务等级协议（SLA）的响应（不同的团队可以设置不停的级别和响应及缓解时间，例如，2~5级不同的影响级别），其中按性质有常见的两种比较大的类型：
- 警告事件：影响服务范围和严重程度较低，如作业性能慢，排队时间长等，或者服务资源消耗到一定阈值等，可以通过当天候命的DRI进行处理。
- 严重事件，如大范围影响平台服务，作业和平台用户使用体验或者数据的，需要及时处理，要求有一定的响应时间要求。如果一定时间内无人响应，将报警需要上升到更高线的响应团队和责任人进行响应。

平台也可以通过设置研发工程师轮班成为直接责任人（Directly Responsible Individuals）简称[DRI](https://about.gitlab.com/handbook/people-group/directly-responsible-individuals/)，通过DRI机制，让开发工程师轮岗进行值班与平台问题修复运维，这样不会产生开发与运维职责脱节，责任到位，热修复（Hotfix）也会执行的更快。这也同样体现当前非常流行的DevOps文化，开发与运维的界限变得越来越模糊，这个本质原因是因为平台软件常常会部署为服务，相当于本身不是部署到客户现场的客户端软件，相当于服务本身的运行和维护责任还是在平台团队，当前很多在线服务都是这样的一种部署方式，也让整个软件工程中DevOps本身变得越来越重要。

### 7.6.5.2 事件管理

对平台产生的系统报警和用户提交的问题事件(Incident)，可以通过一定的事件管理进行维护，并制定一定的SLA，保证指定时间内能够介入，缓解与修复，对高优先级的问题需要有相应策略引入更多人员与团队介入。

<center> <img src="./img/6/7-6-3-operation.png" ch="500" width="800" height="300" /></center>
<center>图7-6-5. 平台事件(Incident)管理(<a href="">图片来源</a>)</center>

如上图所示，平台会通过指定事件(Incident)监测，响应，处理与事后分析的机制，进行事件管理，维护平台的稳定性，应对突发事件，同事通过事件本身不断改进系统这样让平台不断演进。

事件处理通常可以通过以下的方法：
- 人工处理：人工处理从平台设立之初就作为主力方法，同时也是后面基于规则和基于机器学习的处理方法的先验经验与人工数据标注的前提。一般运维人员通过告警日志，性能监控指标和用户直接报警的文字描述，进行初步调查，进一步做一些缓解（例如，封锁服务器），进一步再进行修复（例如重启机器），进而分析完成后可以关闭工单，之后也可以进行事后分析，整理出其他DRI可以使用的经验贴，报告，或者整理为自动化规则或者标注数据，下一次可以自动处理重复类型事件。

- 基于规则处理：需要知识经验积累，根据业务逻辑沉淀出诊断和应对的自动化规则。例如，出现服务器的告警A，则机器上的服务K会级联出现故障，那么就可以归纳为一条规则。同时需要开发针对告警的过滤条件脚本或者服务，根据告警字段自动抽取和匹配，同时还可以应用关联分析，相关性分析等统计方法进一步进行分析，甚至做到因果分析。

- 基于机器学习处理：需要沉淀出一定的有标签数据，或者使用基于无监督的异常检测模型，通常可以互补于基于规则的处理方式。基于机器学习的方式可以定期根据规则的变化进行模型的重新训练，自动更新，比规则方式更加自动化，但是前提是有一定的数据积累，甚至是有标签的数据，对平台要求较高，同时需要运维工程师掌握一定的人工智能知识。

### 7.6.5.3 智能运维（AIOps）

目前对于过于复杂问题，非结构化运维数据，以及希望数据驱动解决的运维问题，也可以考虑智能运维（AIOps）技术进行智能运维，主动介入与预测，互补于常规运维方法。例如，针对云平台上的磁盘损坏历史信息，工作[NTAM WWW '21](https://www.microsoft.com/en-us/research/publication/ntam-neighborhood-temporal-attention-model-for-disk-failure-prediction-in-cloud-platforms/)通过机器学习方式预测磁盘故障，进而让故障较早的发现让服务进行迁移。智能运维技术适合平台规模较大，且运行过一段时间有数据沉淀的团队使用，这样平台本身积累了大量的数据，有条件通过数据驱动（Data Driven）的方法，在一些场景下解放运维人员的工作。目前智能运维在传统大规模云服务和平台有以下场景的应用：
- 故障预测：例如，对服务故障预测与预警，硬件故障预测等。
- 异常检测：例如，对当前的异常进程，异常事件进行检测。
- 故障诊断：例如，对当前事件进行相关性和归因分析。
- 优化系统算法：例如，优化调度算法等。

这类工作本身可以属于AI for System的方向，在之后的章节，还会展开介绍AI for System的内容。

## 小结与讨论

本章我们主要介绍异构计算集群管理系统的中的开发与运维，当前软件工程逐渐由客户端开发到服务开发，平台本身即服务，我们会面临开发以外，部署，运维，诊断等更为棘手的问题，希望读者在读完本章后意识到人工智能基础架构的挑战与复杂性。

## 参考文献
- https://en.wikipedia.org/wiki/Scrum_(software_development)
- https://about.gitlab.com/handbook/people-group/directly-responsible-individuals/
- https://en.wikipedia.org/wiki/DevOps
- https://prometheus.io/docs/alerting/latest/alertmanager/
- https://grafana.com/
- https://prometheus.io/
- http://ganglia.sourceforge.net/
- [Microsoft Online Services incident management](https://www.youtube.com/watch?v=jpTRdV5eZYE)
- https://docs.microsoft.com/zh-cn/azure/architecture/example-scenario/apps/devops-with-aks
- [P. Mattson et al., "MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance," in IEEE Micro, vol. 40, no. 2, pp. 8-16, 1 March-April 2020, doi: 10.1109/MM.2020.2974843.](https://ieeexplore.ieee.org/document/9001257)