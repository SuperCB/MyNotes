<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 异构计算集群调度与资源管理系统 (Heterogeneous Computing Cluster Scheduling and Resource Management System)

# 简介 

随着工业界和学术界大规模应用人工智能技术，大规模和批量的深度学习模型训练需求变得越来越迫切，各家机构投入重金购置和搭建异构计算集群。以配置GPU(Graphics Processing Unit)加速器和InfiniBand网卡的大规模高性能异构计算集群为代表性硬件架构，其中的资源以多租户的形式被组织内的算法工程师使用，用于支撑模型的训练，提供作业，数据与模型的管理，并提供资源隔离。如何高效率与稳定的对资源进行管理，是资源管理系统面对的挑战。

我们回顾一下第一章1.5小节的介绍，异构计算的驱动力又是什么？暗硅 (Dark Silicon)与异构硬件（Heterogeneous Hardware）的趋势：2010年，ARM 的 CTO Mike Muller 将在 EE Times 的Designing with ARM 虚拟会议上发表[暗硅警告（Warns of Dark Silicon）](https://www.eetimes.com/arm-cto-warns-of-dark-silicon/)主题演讲。根据 Mike 的说法，尽管工艺缩小到 11 nm，但固定的功率预算（Fixed Power Budgets ）可能很快就无法利用芯片（Chip）上的所有可用晶体管（Transistors）。如果没有新的创新，设计人员可能会发现自己处于“暗硅 (Dark Silicon)”时代，能够制造出他们无法负担得起的高密度设备。Olivier Temam在2010年ISCA中提到，[一种结果，向异构系统的可能演变：程序分解为“算法序列”，每个算法在任何给定时间映射到一个或几个加速器：晶体管的一小部分在任何给定时间使用（规避“暗硅”问题）](https://pages.saclay.inria.fr/olivier.temam/homepage/ISCA2010web.pdf)。现在的深度学习程序虽然都是在一套Python脚本书写，但是当翻译到底层执行，具体的会拆分到异构设备进行指令执行，的数据加载与作业调度控制在CPU完成，模型训练在GPU完成，也有公司将推理未来部署于FPGA进行加速。当前异构计算的很多工作在数据中心异构和云化的趋势下常常会看到和下面主题相关的系统工作和设计思路：
  -  卸载（Offloading）：对不断增长的计算CPU越来越难以负担，计算卸载到GPU，DPU等，网络传输控制卸载到Infiniband等设备逐渐成为一种行之有效的方式。除去负载的逻辑特点，如果用户的任务负载程序与指令稳定变化小（例如，一些特定场景的模型推理），也较为适合放入 FPGA 中硬件级定制加速器处理逻辑。
  -  分解（Dissaggregation）：数据中希望计算，内存，网络分解（Dissaggregation）提供更加灵活多样适合多样数据中心负载需求的资源配置。
  
以上两点都会造成数据中心的硬件变的越来越异构，同时用户又在多租共享使用硬件，这就产生了抽象与统一管理（Unified Management）的需求。对计算或存储异构硬件，常常抽象在统一的空间内进行管理，最终达到对用户透明。异构计算集群调度与资源管理系统在人工智能系统中类似传统 ***操作系统(Operating System)*** 作用，它对下抽象异构资源（例如，GPU, CPU等），对上层的深度学习作业进行调度和资源分配，在启动作业后也要提供相应的运行时进行资源隔离和环境隔离和生命周期管理。

本章将围绕异构计算集群调度与资源管理系统的运行时，调度，存储，开发与运维内容展开。以期让读者了解，当深度学习生产的问题规模达到多服务器，多租户，多作业的规模，新的平台系统设计挑战和常用解决方案。

# 内容概览

本章包含以下内容：

- [异构计算集群管理系统简介](7.1-异构计算集群管理系统简介.md)
- [作业，镜像与容器](7.2-训练作业，镜像与容器.md)
- [调度](7.3-调度.md)
- [面向深度学习的集群管理系统](7.4-面向深度学习的集群管理系统.md)
- [存储](7.5-存储.md)
- [开发与运维](7.6-开发与运维.md)

