<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 8.2 模型推理的离线优化

<center> <img src="./img/2/8-2-4-offlineopt.png" width="500" height="350" /></center>
<center>图8-2-0. 模型的离线优化 </center>

本章将围绕推理系统或库针对模型的离线优化策略展开相应的介绍，如图所示，其是一般介于工程师训练完模型与运行期推理之间的环节。

- [8.2 模型推理的离线优化](#82-模型推理的离线优化)
  - [8.2.1 通过程序理解推理优化动机](#821-通过程序理解推理优化动机)
  - [8.2.2 推理（Inference）延迟（Latency）](#822-推理inference延迟latency)
  - [8.2.3 层（Layer）间与张量（Tensor）融合](#823-层layer间与张量tensor融合)
  - [8.2.4 目标后端自动调优](#824-目标后端自动调优)
  - [8.2.5 低精度推理与精度校准](#825-低精度推理与精度校准)
  - [8.2.6 模型压缩](#826-模型压缩)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

推理系统类似传统的Web服务，需要响应用户请求，并保证一定的服务等级协议（例如，响应时间低于100ms），进而需要通过一定的策略和优化，保持一定的低延迟（Low Latency），本小节将围绕部署过程中涉及到的延迟问题和相应的优化进行展开。 
总体来说，本章节的优化都在朝着下面的思路设计：
（1）更小的模型：更小的模型意味着更少的浮点运算量和访存开销。例如，低精度推理，模型压缩等。
（2）更小的访存开销：层间与张量融合，目标后端自动调优等。
（3）更大的并行度：目标后端自动调优等。
同时本章的优化属于 ***离线优化*** ，也就是属于部署推理模型之前做的优化，对这段优化处理本身的开销没有严格约束。

## 8.2.1 通过程序理解推理优化动机

请读者在开始后面的内容前，通过以下实例思考相应问题：

我们通过一个矩阵乘法实例进行介绍，下面的代码段中，A和B进行矩阵乘计算，结果存储到result矩阵中。

```python
# 本程序通过循环实现矩阵乘 

# 3x3 矩阵
A = [[0.0,0.0,0.0],
    [4.0,5.0,6.0],
    [7.0,8.0,9.0]]

# 3x4 矩阵
B = [[5.0,8.0,1.0,2.0],
    [6.0,0.0,3.0,0.0],
    [4.0,0.0,0.0,1.0]]

# 3x4 结果矩阵 
result = [[0.0,0.0,0.0,0.0],
         [0.0,0.0,0.0,0.0],
         [0.0,0.0,0.0,0.0]]

# 通过X矩阵的行进行迭代 
for i in range(len(A)):
   # 通过Y矩阵的列进行迭代
   for j in range(len(B[0])):
       # 通过Y矩阵的行进行迭代
       for k in range(len(B)):
           result[i][j] += A[i][k] * B[k][j]
```

1. 程序基准运算量预估：预估整体的[MAC](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation)运算量。
2. 程序基准内存消耗预估：假设A,B,Result的元素数据类型为Float32 （32比特浮点数），预估程序整体的内存占用(Byte字节为单位)。
3. 思考如果元素数据类型换为Int8（8比特整数），内存占用会为多少？MAC会降低为多少？
   - 本问题启示大家思考模型量化的作用和价值
4. 如果底层系统支持元素为0的部分不需要计算直接得到结果为0，那么预估整体的MAC为多少？相比1有多少倍的加速？
   - 本问题启示大家思考模型压缩的作用和价值
5. 如果矩阵乘的循环在函数（func）中如下图所示，设备执行完成后，缓存失效，再执行下一个函数（func）时仍需要加载result和B一次。思考和预估内存到缓存的额外加载代价?
   - 本例启发大家思考内核融合技术优化的作用

```python 
# 模拟未内核融合
def func1(A, B, result1):
    for i in range(len(A)):
        for j in range(len(B[0])):
            for k in range(len(B)):
                result1[i][j] += A[i][k] * B[k][j]
    return result1

def func2(result1, B, result2):
    for i in range(len(B)):
        for j in range(len(B[0])):
                result2[i][j] += result1[i][j] + B[i][j]
    return result2
```

```python 
# 模拟内核融合
def func1(A, B, result1):
    for i in range(len(A)):
        for j in range(len(B[0])):
            for k in range(len(B)):
                result1[i][j] += A[i][k] * B[k][j]

    for i in range(len(B)):
        for j in range(len(B[0])):
                result2[i][j] += result1[i][j] + B[i][j]
    return result2
```
6. 请思考，如果执行此程序的设备缓存线([Cache Line](https://stackoverflow.com/questions/3928995/how-do-cache-lines-work))为3，有2个缓存线，预估缓存未命中率([Cache Misses Rate](https://en.wikipedia.org/wiki/Cache_performance_measurement_and_metric#Introduction_to_types_of_cache_misses))?思考设计缓存失效率最低的访问方式?如果有两个core可以同时启动两个线程计算，之后会如何设计并行计算的策略？
   - 本问题启发读者思考后端代码生成等编译优化策略的作用。

我们对5中func2进行两种访问方式的模拟。
   
```python
# 方式1
# 列访问result1, B
for i in range(len(B)):
    for j in range(len(B[0])):
        result2[i][j] += result1[i][j] + B[i][j]
# 方式2
# 行访问result1，B
for j in range(len(B[0])):
    for i in range(len(B)):
        result2[i][j] += result1[i][j] + B[i][j]
``` 
7. 由于移动端常常使用电池供电，如果我们的功耗和MAC运算量以及访存正相关，思考有什么策略可以减少移动端模型的功耗？
   
那么接下来的内容，我们将围绕代表性的优化策略进行展开。

## 8.2.2 推理（Inference）延迟（Latency）

延迟(Latency)是在客户端给出查询后，推理系统呈现给客户端推理结果所花费的时间。推理服务通常位于关键路径上，因此预测必须既快速同时满足有限的[尾部延迟(Tail Latency)](https://www.cs.utexas.edu/users/mckinley/talks/ds-strangeloop-2017.pdf)约束才能满足服务水平协议(Service Level Agreement)。

例如：某互联网公司的在线服务等级要求(Service Level Agreement): 次秒(Sub-Second)级别延迟服务水平协议。

为了达到低延迟，推理系统面临以下的挑战：
- 交互式应用程序的低延迟需求通常与离线批处理训练框架设计的目标（例如，大规模，高吞吐，大批次等）不一致
- 简单的模型速度快，复杂的模型更加准确，但是浮点运算量更大
- 次秒（Sub-Second）级别延迟约束制了批尺寸（Batch Size）
- 模型融合或多租容易引起长尾延迟（Long Tail Traffic）现象

推理系统常常可以通过以下几个方向进行模型推理的延迟优化：

- 模型优化，降低访存开销：
  - 层（Layer）间融合（Fusion）或张量（Tensor）融合
  - 目标后端自动调优
  - 内存分配策略调优
- 降低一定的准确度，进而降低计算量，最终降低延迟：
  - 低精度推理与精度校准（Precision Calibration）
  - 模型压缩（Model Compression）
- 自适应批尺寸（Batch Size）：动态调整需要进行推理的输入数据数量
- 缓存（Caching）结果：复用已推理的结果或特征



## 8.2.3 层（Layer）间与张量（Tensor）融合

我们将模型抽象为数据流图（Data-Flow Graph)，其中在设备执行的层（Layer）在一些框架内也称作内核（Kernel）或算子（Operator）。我们可以从下图中看到，设备中执行一个内核，一般需要以下几个步骤和时间开销，启动内核，读取数据到设备，执行内核，结果数据写回主存。

<center> <img src="./img/2/8-2-1-fusion.png" width="400" height="140" /></center>
<center>图8-2-1. 内核执行时间线</center>

我们从上面的实例可以观察到需要对模型的层间和张量融合的原因：
- 相对于内核启动开销和每个层的张量数据读写成本，内核(Kernel)计算通常非常快
- 导致内存带宽瓶颈和可用设备(Device)资源的利用不足

我们可以将融合问题抽象为一个优化问题：
- 目标：最小化设备（Device）访存和最大化设备利用率
- 策略：搜索计算图的最优融合策略，降低内核启动，数据访存读写和内存分配释放开销。

如下图所示，左图为未进行融合的深度学习模型网络结构，右图为经过TensorRT优化和融合之后的深度学习模型网络结构。优化后将很多小的层融合为大的层，这样减少了大量的内核启动与数据读写和内存分配释放的开销，提升性能。

<center> <img src="./img/2/8-2-2-fusionopt.png" width="700" height="500" /></center>
<center>图8-2-2. TensorRT使用内核融合进行模型执行优化</center>

代表性的开源应用内核融合优化的系统还有很多，例如，微软开源的[NNFusion](https://github.com/microsoft/nnfusion)。

***经典回顾***

- [内联函数（Inline Function）](https://en.wikipedia.org/wiki/Inline_functiong)：C/C++编程语言中，内联函数用作编译器指令，建议（但不要求）编译器通过执行内联扩展来替换内联函数的主体，即通过在每个函数调用的地址处插入函数代码，从而节省函数的调用开销。 

- [循环融合（Loop Fusion）](https://en.wikipedia.org/wiki/Loop_fission_and_fusion)：循环融合是一种编译器优化和循环转换，它用一个循环替换多个循环。循环融合并不总能提高运行速度。在某些架构上，两个循环实际上可能比一个循环执行得更好，例如，因为每个循环内的数据局部性增加。循环融合的主要好处之一是它可以避免临时内存分配。

- 大数据系统中：Spark中将两个洗牌（Shuffle）阶段之间的用户自定义函数（UDF）自动融合（Fusion）为一个[阶段（Stage）](https://data-flair.training/blogs/spark-stage/)，并根据数据分区（Partition）转换为具体任务（Task）执行，减少UDF之间的数据访存开销。

## 8.2.4 目标后端自动调优

由于深度学习模型中的层的计算逻辑可以转为矩阵运算，而矩阵计算又可以通过循环进行实现。对于循环的实现，由于在不同的推理CPU和硬件设备中，有不同的缓存和内存大小以及访存带宽，进而如何针对不同的设备进行循环的并行化和考虑数据的局部性降低访存开销，可以抽象为一个搜索空间巨大的优化问题。目前有很多深度学习编译器的工作在围绕这个问题展开，它们分别从不同角度入手，有的通过基于专家经验的规则，有的构建代价模型，有的抽象问题为约束优化问题或者通过机器学习自动化学习和预测是常用的方式。由于在编译优化章节对此有较为详细的介绍，本章不再展开相应的内容，读者可以参考编译优化章节内容理解。

代表性的开源后端自动调优编译器或工具有：[TVM](https://tvm.apache.org/),[Hadlide](https://dl.acm.org/doi/10.1145/2491956.2462176),[Ansor](https://tvm.apache.org/2021/03/03/intro-auto-scheduler)等。

<center> <img src="./img/2/8-2-3-autotuning.png" width="1000" height="400" /></center>
<center>图8-2-3. 对比不同后端自动调优搜索策略 <a href="https://arxiv.org/pdf/2006.06762.pdf">图片引用Ansor</a></center>

如图8-2-3所示，当前深度学习算子大多会转换为矩阵计算，而矩阵计算又会转换为多级for循环在特定设备执行。[Ansor](https://arxiv.org/pdf/2006.06762.pdf)工作中总结当前目标后端自动调优通常有以下几种方案：

1. 模板引导搜索（Templated-guided Search）： 如图a所示，在模板引导搜索中，搜索空间由手动模板定义。编译器（例如，TVM）需要用户手动为计算定义编写模板。模板用一些可调参数定义张量程序的结构参数（例如，平铺大小和展开因子）。编译器然后针对输入形状配置和特定的硬件目标搜索这些参数的最佳值。该方法在常见问题上取得了良好的性能，例如，深度学习算子。 然而，开发模板重新需要较大的工作量。
2. 基于顺序构建的搜索（Sequential Construction Based Search）：如图b所示，这种方法通过分解程序结构来细化搜索空间转化为固定的决策序列。然后编译器使用诸如束搜索（Beam Search）之类的算法来搜索好的决策（例如，[Halide](https://dl.acm.org/doi/10.1145/3150211) 自动调度器采用这种方式）。在这种方法中，编译器通过逐步展开计算图中的所有节点顺序构造一个张量程序（Tensor Program）。
3. 分层方法（Hierarchical Approach）：如图c所示，[Ansor](https://arxiv.org/pdf/2006.06762.pdf)由一个分层的搜索空间支持，该空间解耦高级结构和低级细节。Ansor构造自动搜索计算图的空间，无需手动开发模板。 然后从空间中采样完整的程序并执行对完整程序进行微调，避免不准确的预估不完整程序。

***经典回顾***

程序分析（Program Analysis）与编译器（Compiler）领域在过去的几十年中，对循环做了大量的研究与优化，读者可以参考[循环优化（Loop Optimization）](https://en.wikipedia.org/wiki/Loop_optimization)了解。例如：循环分块（Tiling），循环展开（Unrolling），循环交换(Interchange)，循环分解（Fission），循环融合（Fusion）等。而面对如此大的优化空间，当前的工作更多的入手点并不是再找到新的优化点，而是从如何通盘考虑已有循环优化机会，通过更高效或新的算法搜索出近似或超过专家手工优化的效果，以及通过编译器工程上适配更多的设备。

## 8.2.5 低精度推理与精度校准

推理阶段相比训练阶段可以适当降低精度，进而降低推理延迟:
- 大多数深度学习框架都以完整的32位精度（FP32）训练神经网络。
- 使用较低的精度会导致较小的模型大小，较低的内存利用率和延迟以及较高的吞吐量。
- 对模型进行充分训练后，由于不需要进行反向传播求梯度，因此推理计算相比训练常常使用的FP32，推理可以使用半精度FP16甚至INT8张量运算降低计算量和内存占用。

如图所示，不同的精度对应的比特数和取值范围不同，进而产生了不同学习性能和系统性能的权衡。此小节内容将在后续稀疏性章节有更详细的的介绍。

<style>table{margin: auto;}</style>


$$
\begin{array}{|c|c|c|}
    \hline
    & 比特数 & 取值范围 \\
    \hline 
    FP32& 32 & -3.4 \times 10^{38} \sim + 3.4 \times 10^{38} \\
    \hline
    FP16& 16 & -65504 \sim +65504 \\
    \hline 
    INT8 & 8 & -128 \sim +127 \\
    \hline
\end{array}
$$

<center>图8-2-3. 精度取值范围</center>


## 8.2.6 模型压缩

模型压缩（Model Compression）是通过一定的算法和策略，在保证模型预测效果满足一定要求的前提下，尽可能地降低模型权重的大小，进而降低模型的推理计算量，内存开销和模型文件的空间占用，最终降低模型推理延迟。因为其可观的收益和一定的预测效果保证，在模型部署和推理之前，通过模型压缩进行模型精简是常常使用的技术。

我们可以将模型压缩抽象为有约束的优化问题：

优化目标：在所选择的策略下，最小化模型大小
$$min_{policy_i}\{Model\_Size(Policy_i)\}$$

约束：在所选择的策略进行模型压缩后，保证预测精度满足约束

$$Accuracy(Policy_i) \geq Accuracy\_SLA$$

常常使用的模型压缩技术有如下几种：

- 参数裁剪（Parameter Pruning）和共享（Sharing）
  - 剪枝（Pruning）
  - 量化（Quantization）
  - 编码（Encoding）
- 低秩分解（Low-Rank Factorization）
- 知识精炼（Knowledge Distillation）
- …

由于目前有很多手段可以达到模型压缩与量化，那么我们也可以考虑通过集成性工具一站式做好模型压缩（例如，Level, AGP, L1Filter, L2Filter, Slim等剪枝器）与量化（例如，Naive，QAT，BNN等量化器）。例如，通过NNI提供的模型压缩与量化接口实例，我们可以自动化使用多种算法进行尝试和剪枝。下面使用一个简单的[NNI官方示例](https://nni.readthedocs.io/en/v1.6/Compressor/Overview.html)来展示如何修改试用代码以应用压缩算法。假设想使用 Level Pruner 将所有权重修剪为 80% 的稀疏度，可以在训练模型之前将以下三行添加到代码中（这里是[完整的代码](https://github.com/microsoft/nni/tree/master/examples/model_compress)）。函数调用 pruner.compress() 修改用户定义的模型（在 Tensorflow 中，模型可以通过 tf.get_default_graph() 获得，而在 PyTorch 中，模型是定义的模型类），并通过插入遮罩（Masks）修改模型。然后当用户运行模型时，遮罩就会生效。也可以通过算法在运行时调整遮罩。这样当我们面对种类繁多的算法时，可以在框架内一站式解决压缩与量化问题。

通过NNI进行PyTorch 模型压缩实例
```python
from nni.compression.torch import LevelPruner
config_list = [{ 'sparsity': 0.8, 'op_types': ['default'] }]
pruner = LevelPruner(model, config_list)
pruner.compress()
Tensorflow code
```
TensorFlow 实例
```python
from nni.compression.tensorflow import LevelPruner
config_list = [{ 'sparsity': 0.8, 'op_types': ['default'] }]
pruner = LevelPruner(tf.get_default_graph(), config_list)
pruner.compress()
```

在后面章节中我们将有更为细节的模型压缩内容叙述，本小节只介绍与推理相关的模型压缩技术和问题。


## 小结与讨论

本小节主要围绕推理系统的延迟展开讨论，我们总结了低延迟的推理需求，以及围绕这个设计目标，推理系统常常使用的优化策略。优化延迟的目标，受到空间与准确度的约束，深度学习的推理过程相比训练过程在适当情况下可以牺牲一定准确度进而提升延迟。

看完本章内容后，我们可以思考以下几点问题：
层间与张量融合受到哪些约束?
推理和训练优化内存分配策略的侧重点是否有不同？

## 参考文献 

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
- [BLASX: A High Performance Level-3 BLAS Library for Heterogeneous Multi-GPU Computing](https://arxiv.org/abs/1510.05041)