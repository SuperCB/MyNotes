<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->
# 7.2训练作业，镜像与容器

<center> <img src="./img/2/7-2-15-jobrelated.png" ch="500" width="700" height="300" /></center>
<center>图7-2-0. 平台作业与开发体验</center>

集群管理系统对环境依赖和资源隔离问题，需要通过镜像和运行期资源隔离等机制解决。如图7-2-0所示，本章将围绕集群管理系统上运行作业的依赖与运行期资源隔离，以及人工智能作业开发体验进行介绍。

- [7.2训练作业，镜像与容器](#72训练作业镜像与容器)
  - [7.2.1 深度学习作业](#721-深度学习作业)
  - [7.2.2 环境依赖：镜像(Image)](#722-环境依赖镜像image)
  - [7.2.3 运行时资源隔离：容器](#723-运行时资源隔离容器)
    - [从头构建容器理解"进程级虚拟化"](#从头构建容器理解进程级虚拟化)
  - [7.2.4 从操作系统视角看GPU技术栈](#724-从操作系统视角看gpu技术栈)
    - [**GPU技术栈的操作系统抽象**](#gpu技术栈的操作系统抽象)
    - [**观测深度学习作业系统调用(System Call)**](#观测深度学习作业系统调用system-call)
    - [**GPU虚拟化**](#gpu虚拟化)
  - [7.2.5 人工智能作业开发体验（Development Experience）](#725-人工智能作业开发体验development-experience)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

 
## 7.2.1 深度学习作业

当深度学习开发者在本地机器或者独占服务器进行模型的开发与训练时，环境问题较少，还没有暴露更多问题和挑战。例如，下面的几点因素是单机独占环境下的情况：

- 独占环境，无需考虑环境，资源隔离问题。
- 环境依赖路径: 本地/anaconda3。
  - 用户通过Python层的包管理软件或环境变量配置路径，即可完成Python库这层的依赖软件的隔离。
- GPU环境依赖: 本地/usr/local/cuda。
  - 用户本地NVIDIA CUDA等底层库较为固定，也可以通过环境变量较为方便的进行切换。
- 数据路径: 本地/data
  - 用户数据直接上传到服务器本地磁盘，带宽较高。
- 直接执行启动脚本: 存储在服务器磁盘，修改，调试监控等较为方便。

```
# 作业启动脚本
python train.py --batch_size=256  --model_name=resnet50
```

当深度学习作业准备提交到平台，用户需要提供什么信息呢？如图7-2-1和如下程序所示，我们可以参考以下实例提交作业样本模板（实例来源于[OpenPAI](https://github.com/microsoft/pai)），观察相比单机提交需要特殊处理哪些部分和资源。

```
{
    "jobName": "restnet",
    "image": "example.tensorflow:stable",
    "dataDir": "/tmp/data",
    "outputDir": "/tmp/output",
    ...
    "taskRoles": [
        {
            ...
            "taskNumber": 1,
            "cpuNumber": 8,
            "memoryMB": 32768,
            "gpuNumber": 1,
            "command": "python train.py --batch_size=256 \ 
		--model_name=resnet50"
        }
    ]
}
```

<center><img src="img/2/7-2-2-submit.png" ch="300" width="1000" height="380"></center>
<center>图7-2-1. 用户提交的作业规格（Specification）实例，将作业提交到拥有4块GPU的服务器</center>

从作业提交模板中我们主要关注几个方面，并思考平台需要提供怎样的支持。

- 环境依赖：
  - 问题：平台集群中的机器都是相同的操作系统与环境，如何支持用户使用不同的深度学习框架（例如，TensorFlow和PyTorch）和版本？
  - 解决方法：通过"image"填写的Docker镜像名，解决环境依赖问题。用户需要提前将打包好的依赖构建为Docker镜像，并提交到指定的镜像中心，供作业下载。 
- 数据与代码：
  - 问题：平台上一般运行的是深度学习训练作业，每个作业都需要一定的数据作为输入，同时执行相应的代码，如果将数据和代码直接上传会造成接受用户请求的服务器负载过大，同时不能复用已经上传的数据和代码。
  - 解决方法：通过"dataDir"和"outputDir"填写作业依赖的数据和输出路径。用户上传数据和代码到平台指定的文件系统中的相应路径下，未来平台将数据和代码挂载到相应的作业进程。
- 资源申请量：
  - 问题：用户可能会提交运行使用单GPU的作业，多块GPU的作业和分布式的作业，面对多样的用户需求，因为平台无法静态分析用户作业的资源需求，平台需要用户明确告知和配置其资源需求，否则容易提供过多资源浪费，或过少资源造成作业无法启动。
  - 解决方法：用户明确声明需要使用的计算资源（例如，GPU和CPU）和内存，这样让平台根据指定调度策略将匹配的空闲资源进行分配。
- 资源隔离：
  - 问题：用户作业被分配指定资源后，可能多个作业在一台服务器执行，如何保证作业之间尽量不互相干扰？
  - 解决方法：平台可以通过[容器](https://www.docker.com/resources/what-container)/[cgroup](https://en.wikipedia.org/wiki/Cgroups)等技术，将进程进行轻量的资源限定和隔离。
- 任务部署模式：
  - 问题：对于分布式的作业，如果用户不说明，平台无法知道并行化的策略，进而无法确认需要启动的任务数量。需要用户显式的声明。
  - 解决方法：对于分布式的作业，用户要明确告知需要启动的任务数量，进而平台能够启动多个任务副本进行训练。
- 作业启动命令：
  - 问题：当平台给作业分配资源，需要用户告知作业的启动命令，进而平台启动作业，执行相应的代码。
  - 解决方法：用户需要明确在作业中描述相应的代码启动入口命令，进而让作业代码能够启动并执行。

通过以上问题描述和解决方法，相信您已经了解在平台中执行的作业和本地执行的作业差异和需要考虑的问题，我们将在后面章节的内容中，逐步展开其中的重要技术点和原理。

## 7.2.2 环境依赖：镜像(Image)

当用户在平台上执行作业，第一个比较大的问题就是本地开发环境和平台集群环境差异：
- 服务器上没有预装好所需要的个性化环境?
- 不同作业需要的框架，依赖和版本不同，安装繁琐且重复？
- 部署服务器上可能会有大量重复安装的库，占用空间？
- 深度学习特有问题：深度学习作业需要安装CUDA依赖和深度学习框架等
  
所以平台朝着以下目标去选用相应的技术方案解决问题：
复用整体安装环境并创建新环境，层级构建依赖，复用每一层级的依赖。这样既保证能有个性化环境的同时，也能保证性能和资源消耗更小。目前在主流的平台系统中，通过[Docker镜像](https://docs.docker.com/get-started/overview/#docker-objects)来解决这个问题。而Docker镜像的本质是底层通过[Union文件系统(Unionfs)](https://en.wikipedia.org/wiki/UnionFS)等机制而实现，它使得Docker能够高效地存储镜像各层。
Unionfs是Linux、FreeBSD 和 NetBSD 的文件系统服务，其实现了[联合挂载(Union Mount)](https://en.wikipedia.org/wiki/Union_mount)。它允许独立文件系统的文件和目录（称为分支）透明覆盖，形成一个单一的连贯文件系统。在合并的分支中具有相同路径的目录的内容将在新的虚拟文件系统内的单个合并目录中一起看到。在计算机操作系统中，联合挂载(Union Mount)是一种将多个目录组合成一个似乎包含其组合内容的方法。这样的机制使得Docker能够更加高效的存储文件和包。Docker支持多种Unionfs，例如, AUFS, OverlayFS等。

接下来我们看一个实例去理解镜像。

首先，用户需要书写Dockerfile，然后通过Dockerfile中的命令，将构建镜像文件。未来用户可以将文件上传到指定的镜像中心(Docker Hub)，未来平台启动作业后，会下载相应镜像到指定服务器，为作业配置好相应的依赖。

下面就是一个Dockfile实例，我们可以通过其中的注释看到，其实就是在执行在服务器中的安装命令脚本，进行相关库和依赖的安装。

```dockerfile
# 设置镜像使用的基础镜像
FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04
# 设置构建镜像时加入的参数
ARG PYTHON_VERSION=3.6
…
# 构建镜像时所执行的脚本
RUN apt-get update && apt-get install -y --no-install-recommends \
…
RUN curl -o ~/miniconda.sh 
…
 /opt/conda/bin/conda install -y -c pytorch magma-cuda100 && \
…
# 设置指令的工作目录
WORKDIR /opt/pytorch
# 复制文件到镜像中
COPY . .
…
WORKDIR /workspace
RUN chmod -R a+w .
```

经过上面的Dockfile构建后，如图7-2-2，会打包如下的镜像。我们从镜像中可以看到其中的文件就是Dockerfile中所下载和安装的文件。

<center><img src="img/2/7-2-4-pytorchimage.png" ch="300" /></center>
<center>图7-2-2. PyTorch镜像</center>

## 7.2.3 运行时资源隔离：容器

当用户在平台上执行作业，第二个比较大的问题就是用户想像独占资源那样在执行过程中不受其他作业对资源争用产生的干扰：
- 集群资源被共享，如何保证作业互相之间不干扰和多占用资源？
- 如何能够让不同作业可以运行在不同的命名空间防止冲突？
- 如何保证隔离的同时，作业启动的越快越好？
- 深度学习特有问题：GPU的核和内存如何隔离？

所以平台朝着以下目标去选用相应的技术方案解决问题：
能够尽可能细粒度的进行资源隔离，同时减少由于资源隔离(虚拟化)技术造成的新的开销。

通过镜像解决了环境依赖问题，那么运行时的资源隔离问题如何解决呢？目前平台一般通过容器解决，而容器解决资源隔离问题时主要通过控制组（Cgroups）机制解决资源隔离，通过命名空间(Namespace)解决命名空间隔离。

- 容器的定义: 
  - Linux容器（LXC）是一组 1 个或多个与系统其余部分隔离的进程。Linux Containers是一种操作系统级别的虚拟化方法，用于使用单个Linux内核在控制主机上运行多个隔离的Linux系统（容器）。 
- 支撑技术：	
  - 控制组[Cgroups](https://man7.org/linux/man-pages/man7/cgroups.7.html)(缩写自Control Groups)：是一种Linux内核特性，它能够控制(Control)，计数（Accounting）和隔离(Isolation)一组进程的资源（例如，CPU，内存，磁盘I/O，网络等）及使用。
  - 命名空间[Namespace](https://man7.org/linux/man-pages/man7/namespaces.7.html): 命名空间将全局系统资源包装在一个抽象中，使命名空间内的进程看起来他们拥有自己独立的全局资源实例。命名空间可以实现如，pid，net，mnt，ipc，user等的包装和抽象。

如图7-2-3所示，用户可以通过创建控制组(Cgroups)进行资源的限制，之后启动进程时，通过控制组(Cgroups)约束资源使用量，并在运行时进行隔离。

<center><img src="img/2/7-2-8-cgroups.png" ch="300" width="500" height="300"/></center>
<center>图7-2-3. 控制组实例</center>

如图7-2-4所示，用户可以通过创建命名空间(Namespaces)进行资源的包装，之后启动进程时，进程内只能看到命名空间内的资源，无法感知其进程外主机其他的资源。

<center><img src="img/2/7-2-7-namespace.png" ch="300" width="500" height="500"/></center>
<center>图7-2-4. 命名空间实例</center>

由于深度学习目前依赖GPU进行训练，为了让容器能支持挂载GPU，一般GPU的厂商都会提供对Docker的特定支持来提供相应的功能。例如，NVIDIA提供了针对NVDIA GPU的支持，用户可以参考官方[nvidia-docker文档](https://github.com/NVIDIA/nvidia-docker)进行环境配置。但是由于软硬件的虚拟化支持不像传统CPU充分，所以目前主流方式还是GPU为粒度的挂载和隔离，无法原生像传统操作系统对CPU进行细粒度的时分复用、内存隔离和动态迁移。
当完成环境配置之后，用户可以使用挂载NVIDIA GPU的Docker容器实例。我们可以通过以下命令理解平台是如何启动GPU的容器。

```
# 通过官方CUDA镜像测试nvidia-smi命令，并挂载所有GPU
$ docker run --gpus all nvidia/cuda:9.0-base nvidia-smi

# 启动一个可以访问GPU的容器，并挂载2块GPU
$ docker run --gpus 2 nvidia/cuda:9.0-base nvidia-smi

# 启动一个可以访问GPU的容器，并挂载1号和2号GPU
$ docker run --gpus '"device=1,2"' nvidia/cuda:9.0-base nvidia-smi
```

### 从头构建容器理解"进程级虚拟化"

如果大家对容器底层是如何构建的有更深入的了解感兴趣，可以参考实例[通过Go语言从头构建容器](https://medium.com/swlh/build-containers-from-scratch-in-go-part-1-namespaces-c07d2291038b)，[Containers from Scratch](https://ericchiang.github.io/post/containers-from-scratch/)。
通过这个实例，读者可以了解容器的底层是通过哪些调用所构建出来，同时用户可以构建自己的个性化容器。接下来，我们通过[Containers from Scratch](https://ericchiang.github.io/post/containers-from-scratch/)实例，构建一个容器，这样让读者从抽象的概念中跳出，由具体的实例感知，为何容器被称作"进程级虚拟化"，同时读者就可以思考相比于基于Hypervisor的虚拟机其优劣势。同时理解Docker本身是一个技术泛称，同时是一家公司，其本身更多是提供工具链，镜像中心，标准。容器和镜像我们可以通过Linux系统调用（System Call）和命令从头构建。

从头构建一个容器实例与步骤，这个实例对理解Docker镜像和容器的底层机制有较好的帮助。读者可以参考[Containers from Scratch](https://ericchiang.github.io/post/containers-from-scratch/)及以下步骤进行构建：

1. 设置容器文件系统
    容器镜像，其实很多时候是tar文件，我们可以从网上下载一个简单的tar文件，这里包含Debian文件系统。
    ```shell
    $ wget https://github.com/ericchiang/containers-from-scratch/releases/download/v0.1.0/rootfs.tar.gz
    $ sha256sum rootfs.tar.gz 
    c79bfb46b9cf842055761a49161831aee8f4e667ad9e84ab57ab324a49bc828c  rootfs.tar.gz
    ```
    解压并观察压缩文件内容，解压后的文件夹像一个Linux操作系统。对如何构建这个tar文件，读者可以参考其他实例和资料，当前我们暂不在此实例介绍。
    ```shell
    $ sudo tar -zxf rootfs.tar.gz
    $ ls rootfs
    bin   dev  home  lib64  mnt  proc  run   srv  tmp  var
    boot  etc  lib   media  opt  root  sbin  sys  usr
    ```
2. [chroot](https://man7.org/linux/man-pages/man2/chroot.2.html)构建进程文件夹系统视图
    我们将使用的第一个工具是 chroot。一个围绕类似名称的系统调用的瘦包装器，它允许我们限制文件系统的进程视图。 在这种情况下，我们将进程限制在“rootfs”目录，然后执行一个 shell（/bin/bash）。
    ```shell
    $ sudo chroot rootfs /bin/bash
    root@localhost:/# ls /
    bin   dev  home  lib64  mnt  proc  run   srv  tmp  var
    boot  etc  lib   media  opt  root  sbin  sys  usr
    root@localhost:/# which python
    /usr/bin/python
    root@localhost:/# /usr/bin/python -c 'print "Hello, container world!"'
    Hello, container world!
    root@localhost:/# 
    ```
    1，2步骤等价于我们拉取和挂载Docker镜像。
3. [unshare](https://man7.org/linux/man-pages/man1/unshare.1.html)创建命名空间
   通过unshare让这个 chroot 进程命名空间隔离，让我们在另一个终端的主机上运行命令。命名空间允许我们创建系统的受限视图，例如进程树、网络接口和挂载。创建命名空间不难，只需一个带有一个参数的系统调用，unshare。 unshare 命令行工具为我们提供了一个很好的系统调用包装器，让我们手动设置命名空间。 如下脚本所示，我们将为 shell 创建一个 PID 命名空间，然后像上一个示例一样执行 chroot。
   ```shell
    $ sudo unshare -p -f --mount-proc=$PWD/rootfs/proc \
    chroot rootfs /bin/bash
    root@localhost:/# ps aux
    USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
    root         1  0.0  0.0  20268  3240 ?        S    22:34   0:00 /bin/bash
    root         2  0.0  0.0  17504  2096 ?        R+   22:34   0:00 ps aux
    root@localhost:/#
   ```
   在创建了一个新的进程命名空间之后，在我们的 chroot 内一探究竟，我们会发现一些有趣的东西。shell认为它的PID是1，更重要的是，我们再也看不到主机的进程树了。
4. [nsenter](https://man7.org/linux/man-pages/man1/nsenter.1.html)进入命名空间
   此步骤等价于我们查询Docker容器ID后，通过Docker exec进入容
   器。
   类似的我们需要先找到进程ID和命名空间，之后再进入这个命名空间。
   通过下面命令找到刚才我们通过chroot运行的shell进程。
   ```shell
    $ # From the host, not the chroot.
    $ ps aux | grep /bin/bash | grep root
    ...
    root     29840  0.0  0.0  20272  3064 pts/5    S+   17:25   0:00 /bin/bash
   ```
   内核将 /proc/(PID)/ns 下的命名空间公开为文件。 在这种情况下，/proc/29840/ns/pid 是我们希望加入的进程命名空间。
   ```shell
    $ sudo ls -l /proc/29840/ns
    total 0
    lrwxrwxrwx. 1 root root 0 Oct 15 17:31 ipc -> 'ipc:[4026531839]'
    lrwxrwxrwx. 1 root root 0 Oct 15 17:31 mnt -> 'mnt:[4026532434]'
    lrwxrwxrwx. 1 root root 0 Oct 15 17:31 net -> 'net:[4026531969]'
    lrwxrwxrwx. 1 root root 0 Oct 15 17:31 pid -> 'pid:[4026532446]'
    lrwxrwxrwx. 1 root root 0 Oct 15 17:31 user -> 'user:[4026531837]'
    lrwxrwxrwx. 1 root root 0 Oct 15 17:31 uts -> 'uts:[4026531838]'
   ```
   nsenter 命令提供了一个围绕 setns 的包装器以进入命名空间。 我们将提供命名空间文件，然后运行 unshare 以重新挂载 /proc 和 chroot 以设置 chroot。 这一次，我们的 shell 将加入现有的命名空间，而不是创建一个新的命名空间。
   ```shell
    $ sudo nsenter --pid=/proc/29840/ns/pid \
    unshare -f --mount-proc=$PWD/rootfs/proc \
    chroot rootfs /bin/bash
    root@localhost:/# ps aux
    USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
    root         1  0.0  0.0  20272  3064 ?        S+   00:25   0:00 /bin/bash
    root         5  0.0  0.0  20276  3248 ?        S    00:29   0:00 /bin/bash
    root         6  0.0  0.0  17504  1984 ?        R+   00:30   0:00 ps aux
   ```
5. [cgroups](https://man7.org/linux/man-pages/man7/cgroups.7.html)进行资源约束
    对于这个例子，我们将创建一个 cgroup 来限制进程的内存。 创建 cgroup 很简单，只需创建一个目录即可。 在这种情况下，我们将创建一个名为“demo”的内存组。 创建后，内核将使用可用于配置 cgroup 的文件填充目录。
    ```shell
    $ sudo su
    $ mkdir /sys/fs/cgroup/memory/demo
    $ ls /sys/fs/cgroup/memory/demo/
    cgroup.clone_children               memory.memsw.failcnt
    cgroup.event_control                memory.memsw.limit_in_bytes
    cgroup.procs                        memory.memsw.max_usage_in_bytes
    memory.failcnt                      memory.memsw.usage_in_bytes
    memory.force_empty                  memory.move_charge_at_immigrate
    memory.kmem.failcnt                 memory.numa_stat
    memory.kmem.limit_in_bytes          memory.oom_control
    memory.kmem.max_usage_in_bytes      memory.pressure_level
    memory.kmem.slabinfo                memory.soft_limit_in_bytes
    memory.kmem.tcp.failcnt             memory.stat
    memory.kmem.tcp.limit_in_bytes      memory.swappiness
    memory.kmem.tcp.max_usage_in_bytes  memory.usage_in_bytes
    memory.kmem.tcp.usage_in_bytes      memory.use_hierarchy
    memory.kmem.usage_in_bytes          notify_on_release
    memory.limit_in_bytes               tasks
    memory.max_usage_in_bytes
    ```
    要调整一个值，我们只需写入相应的文件。 让我们将 cgroup 限制为 100MB 内存并关闭交换。
    ```shell
    $ echo "100000000" > /sys/fs/cgroup/memory/demo/memory.imit_in_bytes
    $ echo "0" > /sys/fs/cgroup/memory/demo/memory.swappiness
    ```
    任务文件很特殊，它包含分配给 cgroup 的进程列表。 要加入 cgroup，我们可以编写自己的 PID。读者可以替换为上面程序的PID 29840即可。
    ```shell
    $ echo 29840 > /sys/fs/cgroup/memory/demo/tasks
    ```

通过以上实例，相信读者已经理解容器和镜像的工作原理，我们忽略很多其他安全，优化和功能，以期读者了解核心问题。

## 7.2.4 从操作系统视角看GPU技术栈

GPU在当前计算机中被抽象为设备，操作系统通过ioctl系统调用进行设备控制，如图7-2-5所示。在以往的工作中，有两个方向的思路尝试去将GPU技术栈纳入到操作系统的管理中并提供多租，虚拟化等的支持。

<center> <img src="./img/2/7-2-9-osgpu.png" ch="500" width="700" height="400" /></center>
<center>图7-2-5. CPU和GPU技术栈与操作系统抽象(<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2011/05/hotos11-final.pdf">图片来源</a>)</center>

从图7-2-5中可以看到，CPU程序中有大量的系统调用支持，操作系统提供对各种硬件抽象管理与用户多进程多租的支持。但是对GPU程序，当前模式更像是Client/Server抽象，GPU是设备，CPU程序提交作业到GPU并获取响应结果，但是对操作系统GPU本身是一个黑盒，一般通过[ioctl](https://man7.org/linux/man-pages/man2/ioctl.2.html)进行有限的交互与控制。

### **GPU技术栈的操作系统抽象**

1. 思路1：操作系统纳入GPU。让GPU能被ioctl以外的系统调用所管理，或者操作系统理解ioctl语义对其能做细粒度资源管理。
   - 当前思路依赖设备供应商在驱动层暴露更多信息被操作系统管理，但是之前一些厂商（例如，英伟达）这部分为闭源，且只能通过ioctl控制设备，很难分析语义。2011年HotOS上，Christopher J. Rossbach等人在[Operating Systems must support GPU abstractions
](https://www.microsoft.com/en-us/research/wp-content/uploads/2011/05/hotos11-final.pdf)曾有相关工作提出在操作系统层面抽象GPU管理，但是这篇工作只提出构想，并受限于设备供应商的设备驱动的支持，从之后的发展状况来看，受限于设备供应商的配合和支持，当前道路并不没有走通。在ATC13，Konstantinos Menychtas等人提出[Enabling OS Research by Inferring Interactions in the Black-Box GPU Stack](https://www.usenix.org/system/files/conference/atc13/atc13-menychtas.pdf)当前GPU资源是黑盒，不能被操作系统所管理，所以对系统调用进行拦截和刻画抽象出程序状态机，启发后续操作系统对GPU设备的研究。之后的工作也层尝试设计从GPU调用操作系统系统调用（[ISCA18 Generic System Calls for GPUs](http://www.cs.yale.edu/homes/abhishek/jvesely-isca18.pdf)）。但是以上的工作由于实用性，部署难度和受限于厂商支持，在后续的业界平台或研究工作中没有过多的跟随工作。2022年5月，[NVIDIA开源了其GPU内核模块（GPU Kernel Modules）](https://developer.nvidia.com/blog/nvidia-releases-open-source-gpu-kernel-modules/)，一定程度上会让当前此方向研究有更新的进展。
2. 思路2：GPU编程接口上抽象操作系统。此类工作以设备API作为系统调用的视角抽象设备管理，在设备API层以下进行拦截或管理达到多租或虚拟化的效果。
   - 厂商官方工作：[MPS](https://docs.nvidia.com/deploy/mps/index.html)，[Unified Memory](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)等工作是NVIDIA将传统操作系统的多进程调度、虚拟内存等经典设计思路在CUDA上进行的实现，进而能够实现复用GPU，打破GPU显存瓶颈的效果。
   - 研究工作：以[rCUDA](http://www.rcuda.net/)，[Singularity](https://arxiv.org/abs/2202.07848)为代表的工作在[NVIDIA Runtime API](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html)，[Driver API](https://docs.nvidia.com/cuda/cuda-driver-api/index.html)上进行拦截，提供时分复用，内存隔离，和动态迁移的功能。
   - 第三方公司的工作：以[OrionX猎户座](https://blog.csdn.net/m0_49711991/article/details/107979798)为代表的产品，设计类似传统操作系统的内核态抽象，在大规模集群上提供资源池化，并通过内核态抽象让系统更安全和稳定。

### **观测深度学习作业系统调用(System Call)**

[strace](https://strace.io/)
是用于 Linux 的诊断、调试和指导性用户空间实用程序。它用于监视进程与 Linux 内核之间的交互，包括系统调用、信号传递和进程状态的变化。本小节我们可以通过strace观测深度学习程序如何在操作系统的系统调用层和GPU打交道，以另一个抽象层次和视角深入理解深度学习程序的底层原理。
1. 例如在Ubuntu操作系统中，执行一下安装命令：
```
apt-get install strace
```
2. 假设用户有一个训练程序（train.py），里面是PyTorch训练卷积神经网络。

```
strace python train.py
```
3. 观测系统调用日志。

我们可以观察主要调用的是ioctl和mmap等系统调用，但是从ioctl很难获取到可读和可解释信息。在之前ATC13研究工作[Enabling OS Research by Inferring Interactions
in the Black-Box GPU Stack](https://www.usenix.org/system/files/conference/atc13/atc13-menychtas.pdf)中也有对ioctl进行收集分析并抽象出状态机的工作。

### **GPU虚拟化**

目前代表性的GPU虚拟化资源隔离技术有以下几种：

- 应用程序编程接口远程处理(API Remoting)技术：
  - 包装GPU APIs作为客户前端，通过一个转发层作为后端，协调所有对GPU的访问。挑战之一在于要最小化前端和后端的通信代价，同时API转发面临着充分支持的挑战功能，由于侵入的复杂性修改客户图形软件堆栈，以及客户和主机图形之间不兼容软件栈。
  - 代表性工作：[GVirtuS](https://github.com/cjg/GVirtuS), [vCUDA](https://github.com/tkestack/vcuda-controller), [rCUDA](http://www.rcuda.net/), [qCUDA](https://github.com/coldfunction/qCUDA)。
- 直接GPU直通(Direct GPU Pass-Through)技术：
  - 在Direct GPU pass-through技术中，GPU被单个虚拟机独占且永久地直接访问，其实现的GPU资源隔离粒度为单块GPU，不支持热迁移。GPU直通是一种允许Linux内核直接将内部GPU呈现给虚拟机的技术，该技术实现了96-100%的本地性能，但GPU提供的加速不能在多个虚拟机之间共享。
  - 代表性工作：例如，NIVIDIA对公有云厂商[Amazon AWS, Microsoft Azure, Google Cloud Platform, Alibaba Cloud](https://docs.nvidia.com/grid/cloud-service-support.html)提供GPU pass-through技术等。
- [中介直通(Mediated Pass-Through)](https://www.usenix.org/system/files/conference/atc14/atc14-paper-tian.pdf)技术：
  - 直通传递性能关键型资源和访问，而在设备上中介代理(Mediating)特权操作，使用性能好，功能齐全，共享能力强。
  - 代表性工作：NIVIDIA对公有云厂商[Amazon AWS, Microsoft Azure, Google Cloud Platform, Alibaba Cloud](https://docs.nvidia.com/grid/cloud-service-support.html)提供的vGPU技术. 
- 设备仿真(Device Emulation)技术：
  - GPU架构非常复杂，变化很快，它们的内部细节通常是保密的。完全虚拟化新一代的GPU通常是不可行，只有旧的和更简单的一代才容易通过emulation的方式虚拟化。并且其实现非常复杂性能极低，所以不符合今天的要求的需求。
  - 代表性工作：[VMware SVGA 3D software renderer](https://techzone.vmware.com/resource/deploying-hardware-accelerated-graphics-vmware-horizon-7), [VirtualBox VMSVGA graphics controller](https://www.virtualbox.org/manual/)等。

由于目前一般训练平台中部署的大多数GPU为NVIDIA GPU，其提供的最新的代表性GPU资源隔离技术有以下几种：
- [NVIDIA 多实例GPU（Multi-Instance GPU）简称 (MIG)](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)：多实例GPU是NVIDIA 在Ampe其在最新的Ampere系列GPU中开始支持的，在硬件层面将GPU实例进行隔离与虚拟化的技术。多实例 GPU (MIG) 可扩展每个NVIDIA A100 GPU的共享能力和利用率。MIG可将 A100 和 H100 GPU 划分为最多达七个实例，每个实例均与各自的高带宽显存、缓存和计算核心完全隔离。
- [NVIDIA 多进程服务（Multi-Process Service）简称（MPS）](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)：多进程服务(MPS)是NVIDIA提供的软件层共享物理GPU资源并提供一定程度进程级虚拟化的技术。MPS运行时架构旨在透明地支持多进程CUDA应用程序，通常是MPI作业。其利用[Hyper-Q](https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf)功能实现，Hyper-Q允许CUDA内核在同一GPU上并发处理，当GPU计算能力未被单个应用进程充分利用时，这可以提高性能。MPS一般在一些GPU型号中如果使用的是软件层的实现有一定开销，如果是硬件层的实现能够减少开销。例如，研究工作[Gandiva](https://www.usenix.org/system/files/osdi18-xiao.pdf)发现，MPS会导致P40/P100的较大开销和性能损失。然而，[V100](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf) 中提供对MPS的硬件支持，使用MPS可能能够在V100减少软件层支持方式产生开销并提升利用率。

## 7.2.5 人工智能作业开发体验（Development Experience）

在使用集群管理系统时，通常人工智能算法工程师可以使用以下模式的开发环境进行人工智能作业与Python脚本的开发。
- 作业在提交前，可以选用下面的工具进行Python作业的开发。
- 一般作业提交到平台前用户需要填写作业的规格（Specification），进而选用下面的工具进行提交，监控与管理。

以下工具有些只能进行作业提交，有些能进行Python开发，有些则两个功能都兼顾：
- 客户端集成开发环境（Integrated Development Environment）
  - [Visual Studio Code](https://code.visualstudio.com/)：Visual Studio Code，通常也称为 VS Code，是 Microsoft 为 Windows、Linux 和 macOS 开发的源代码编辑器。在人工智能场景下，算法工程师主要使用VS Code进行Python开发，调试（Debugging）、语法高亮（Syntax Highlighting）、智能代码完成 （Intelligent Code Completion）、预装代码片段（Code Snippet）、代码重构（Code Refactoring）和嵌入 Git 的支持。用户可以更改主题、键盘快捷键、首选项，并安装添加额外功能的扩展。使用VS Code类似的客户端IDE进行开发的特点是，功能强大，调试，补全等功能完善。
  - VS Code实用人工智能插件简介：
    - 一站式开发插件：[Tools for AI](https://github.com/microsoft/vscode-tools-for-ai) 当前已改名 Visual Studio Code Azure 机器学习扩展。可以使用 Visual Studio Code 界面中的 Azure 机器学习服务轻松构建、训练和部署机器学习模型到云或边缘。 此扩展的早期版本以 Visual Studio Code Tools for AI 的名称发布。工具提供，部署到云端或边缘的支持，常用深度学习库的支持，本地实验再部署到大规模集群，集成自动化机器学习，通过CI/CD工具跟踪实验，管理模型。
    - 代码完成（Code Completion）：[Kite for VS Code](https://www.kite.com/integrations/vs-code/) 适用于 VS Code 的各种语言，通过海量代码库，训练人工智能模型，提供智能代码完成服务。智能感知（Intellisense）、代码片段（Code Snippets）、光标跟随（Cursor-Following）文档的 AI 代码完成。 Kite 支持 Python 等文件类型，如图7-2-6.对比有代码补全将大幅提升开发生产力，这也是集成开发环境的优势。同时我们也看到这类工作属于AI for System的一种，OpenAI也开源[Copilot](https://copilot.github.com/)进行更加智能化的代码提示和程序合成（Program Synthesis）。
  <center> <img src="./img/2/../2/7-2-11-codecompletion.png" ch="500" width="400" height="200" /></center>
<center>图7-2-6. Kite对Python代码补全 (<a href="https://www.kite.com/">图片来源</a>)</center>

- 网页（Web）服务开发环境
  - 集群提供的Web管理界面：一般平台会提供一个Web页面方便用户提交作业的规格（Specification），填写好作业镜像，启动命令，资源等，如前面小节所示。网页服务开发环境适合提交与管理，监控作业。图7-2-7.所示，用户可以通过OpenPAI的提交界面，填写，作业名，需要提交的虚拟集群（Virtual Cluster），打包和上传好的镜像名，资源需求和启动命令，即可完成提交。
  - [Jupyter Notebook](https://jupyter.org/)：有些平台会提供Jupyter服务并进行运维，但是Jupter特点是一般用户提交作业独占资源后使用或者是交互式作业，如果单纯是用Jupter做为作业提交入口，不容易做权限管理与作业运维。对用户来说是不需要安装本地环境，较为轻量。
- 命令行：平台一般也可以提供命令行工具。命令行的方式也是用户比较习惯使用的方式，其特点是轻量，容易批量自动化的提交作业，且方便未来复用模板。命令行只适合提交与管理作业。
- REST API：平台一般也可以提供REST API。REST API作为作业提交或监控作业的接口好处是，可编程，且不需要提前安装额外库，比命令行本身还要轻量，缺点是需要用户做一定的调用与结果解析编程。REST API只适合提交与管理作业。

<center> <img src="./img/2/../2/7-2-10-submitjob.png" ch="500" width="1200" height="600" /></center>
<center>图7-2-7. 通过Web界面提交作业到集群 (<a href="https://github.com/microsoft/pai/blob/master/docs/manual/cluster-user/imgs/new-input-command.png">图片来源</a>)</center> 


如图7-2-8所示，开发者在提交作业到集群之前一般会经历大致三个环境的开发，因为集群资源紧张需要排队，一般开发者会先在自己的开发环境开发完成保证程序不出错后再提交到集群平台。

1. 书写Python人工智能程序：用户可以本地使用VS Code等工具进行书写。VS Code通过插件方便本地调试，代码静态检测，与代码完成，能比较好的提升开发效率，同时不需要平台资源排队，对快速开发原型非常方便。
2. 本地一般没有GPU或者GPU资源不足以训练模型，一般如果有可用的测试服务器挂载有GPU，开发者可以在第二个阶段提交作业到测试服务器进行测试，如果是小规模作业也可以在服务器完成一定的训练，但是GPU不足以满足大规模多卡和分布式训练，或者搜索空间巨大的超参数搜索需求，所以调试完成的程序可以在测试服务器进行Docker镜像构建或者上传数据等。在这个阶段比较适合使用VS Code配合[Remote SSH](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh)插件进行远程开发。
3. 当用户确保程序开发完成就可以将作业提交（用户可以选用命令行工具，Web或者REST API进行作业提交）到平台进行批处理作业的执行，由于GPU是稀缺资源，可能会经历一定的排队的流程，这个时候用户可以用闲暇时间继续开发新的作业或者阅读论文等找新的优化点与调试其他作业与模型。作业提交之后就可以参考如图7-2-9的流程，进行作业的完整执行了。这个过程就比较适合通过Web访问作业监控界面，SSH登陆到作业进行调试，或者通过部署Jupyter进行交互式开发。
<center> <img src="./img/2/../2/7-2-13-devflow.png" ch="500" width="700" height="300" /></center>
<center>图7-2-8. 人工智能作业的不同开发环境 (<a href="">图片来源</a>)</center>

如图7-2-9所示，开发者一般经历本地开发，测试服务器开发，进而再打包程序提交到平台，监控作业状态与调试，最终程序执行完成返回状态，模型或数据，开发者可以再开始下一次的作业提交。

<center> <img src="./img/2/../2/7-2-14-devlifesequence.png" ch="500" width="1100" height="750" /></center>
<center>图7-2-9. 人工智能作业开发体验时序图 (<a href="">图片来源</a>)</center>

如图7-2-10所示，当作业已经调试完成，提交到集群上之后，会经历以下的生命周期完成训练过程。
1. 用户首先上传数据到存储。
2. 上传镜像到镜像中心。
3. 提交作业规格，填写数据，镜像路径，资源需求和启动命令行。
4. 集群调度器调度作业。
5. 空闲GPU节点拉取（Pull）镜像。
6. 空闲GPU节点启动作业。
7. 挂载文件系统。
8. 作业运行启动。
9. 作业监控不断汇报性能指标和日志用于观测与调试。
10. 训练完成作业保存结果。

<center> <img src="./img/2/../2/7-2-12-joblifecycle.png" ch="500" width="1100" height="600" /></center>
<center>图7-2-10. 提交到集群的作业生命周期 (<a href="">图片来源</a>)</center>


## 小结与讨论

本章我们主要介绍异构计算集群管理系统的运行时，在运行时中，资源与环境隔离是核心问题，虽然我们看到有很多前沿的公司与开源软件在视图解决相应问题，但是其底层更彻底的解决方案还是基于操作系统和硬件的底层原语的支持。容器与镜像解决了环境依赖，资源隔离进而奠定未来平台提供多租的基石。

请读者思考，作业运行期在GPU技术栈下面临了什么新的问题？
相比传统操作系统，在GPU技术栈还不完善的功能是？
  
## 参考文献 

- https://man7.org/linux/man-pages/man7/cgroups.7.html
- https://man7.org/linux/man-pages/man7/namespaces.7.html
- https://www.docker.com/
- https://docs.docker.com/engine/reference/commandline/image/
- https://hub.docker.com/
- https://www.docker.com/resources/what-container
- https://github.com/NVIDIA/nvidia-docker 
- Use the AUFS storage driver
- Making Containers More Isolated: An Overview of Sandboxed Container Technologies
- Jörg Thalheim, Pramod Bhatotia, Pedro Fonseca, and Baris Kasikci. 2018. CNTR: lightweight OS containers. In Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC '18). USENIX Association, USA, 199–212.
- Edward Oakes, Leon Yang, Dennis Zhou, Kevin Houck, Tyler Harter, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. 2018. SOCK: rapid task provisioning with serverless-optimized containers. In Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC '18). USENIX Association, USA, 57–69.
- https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf
- https://www.nvidia.com/en-us/technologies/multi-instance-gpu/
- http://www.rcuda.net/
- https://www.slideshare.net/knoldus/union-filesystem-a-building-blocks-of-a-container
- https://www.nvidia.com/en-us/technologies/multi-instance-gpu/
- https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf
- https://en.wikipedia.org/wiki/GPU_virtualization
- https://slidetodoc.com/gpu-virtualization-on-vmwares-hosted-io-architecture-micah/
- https://www.usenix.org/system/files/conference/atc14/atc14-paper-tian.pdf
