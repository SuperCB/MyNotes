<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 8.1 推理系统简介

本章将围绕推理系统的应用场景，对比推理和训练场景的不同点，进而介绍推理系统的设计目标与约束，为后面章节内容的展开做好铺垫。

- [8.1 推理系统简介](#81-推理系统简介)
  - [8.1.1 对比推理与训练过程](#811-对比推理与训练过程)
  - [8.1.2 推理系统的优化目标与约束](#812-推理系统的优化目标与约束)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)


## 8.1.1 对比推理与训练过程

深度学习模型已经广泛的部署在各类应用中，充斥在我们生活的周边。例如，对话机器人，新闻推荐系统，物体检测器等。这些应用中，部署深度学习模型。

<center> <img src="./img/1/8-1-1-app.png" width="500" height="180" /></center>
<center>图8-1-1. 典型深度学习推理应用</center>

如图8-1-1所示，对话机器人接收一句问题作为输入，通过深度学习模型（例如，Seq2Seq模型）预测出回答并返回给用户。在这个过程中，推理系统需要考虑和提供以下的功能：
- 提供可以被用户调用的接口
- 能够完成一定的数据处理将输入数据转为向量
- 能够在指定低延迟要求下返回用户响应
- 能够利用多样的加速器进行一定的加速
- 能够随着用户的增长保持高吞吐的服务响应和动态进行扩容
- 能够可靠的提供服务，应对软硬件的失效
- 能够支持算法工程师不断更新迭代模型，应对不断变化的新框架
- ...

我们通过上面可以看到，单纯复用原有的Web服务器或者移动端应用软件，只能解决其中一部分问题，在深度学习模型推理的场景下，产生了新的系统设计需求与挑战。

接下来，我们先从深度学习训练过程和推理过程对比两者的相同点和不同点，以及在生命周期所处的环节，进而便于理解深度学习推理系统所侧重的目标。

如图8-1-2，深度学习模型的生命周期（Life Cycle）包含以下几个阶段：首先，需要从存储系统或其他数据源收集和准备训练数据。之后进入训练阶段，开始训练模型，一旦模型满足一定的学习性能，终止训练，并保存模型。完成训练之后，模型部署在推理系统中。推理系统对外暴露接口，接收用户请求或系统调用，模型通过推理（Inference）处理完请求后，返回给用户相应的响应结果，完成推理任务。

<center> <img src="./img/1/8-1-2-lifecycle.png" width="500" height="200" /></center>
<center>图8-1-2. 深度学习模型的生命周期</center>

如果我们将训练阶段和推理阶段模型的执行过程进行抽象，我们可以总结出两者的共性和不同。训练阶段，深度学习模型常常采用梯度下降算法或类似的优化算法进行模型训练，我们可以将其拆解为三个阶段:
- 前向传播（Forward Propagation）过程将输入样本（Sample）计算为输出标签（Label）。
- 反向传播（Back Propagation）过程求解权重的梯度。
- 梯度更新（Weight Update）阶段将模型权重通过一定的步长和梯度进行更新。在推理阶段，则只需要执行前向传播过程，将输入样本计算为输出标签。

如图8-1-3，相比训练阶段，推理阶段则只需要执行前向传播（Forward Propagation）一个过程，将输入样本通过深度学习模型计算输出标签（Label）。

<center> <img src="./img/1/8-1-3-traininfer.png" width="500" height="300" /></center>
<center>图8-1-3. 模型训练与推理阶段</center>

训练作业常常在之前章节中所介绍的异构集群管理系统中进行执行，通常执行，几个小时，天，或周，是类似传统的批处理作业（Batch Job）。而推理作业需要 $ 7 \times 24 $小时运行，类似传统的在线服务(Online Service)。具体来说，深度学习模型推理相比训练的新特点与挑战主要有以下几点：

- 模型被部署为长期运行的服务
  - 服务有明确对请求的低延迟（Low Latency）和高吞吐（High Throughput）需求
- 推理有更苛刻的资源约束
  - 更小的内存，更低的功耗等
- 推理不需要反向传播梯度下降
  - 可以牺牲一定的数据精度
- 部署的设备型号更加多样
  - 需要多样的定制化的优化

<center> <img src="./img/1/8-1-10-comparetraininfer.png" width="800" height="300" /></center>
<center>图8-1-4. 模型训练与推理对比 (<a href="https://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf">图片来源</a>)</center>

如图8-1-4，深度学习训练与推理相比。在训练中，需要许多输入，通常是更大的批尺寸（Batch Size），用于训练深度学习模型。 在推理中，数据经过训练的网络用于发现和预测新输入中的信息，这些输入通过更小的批尺寸输入网络。

对于推理，相比训练的性能目标是不同的。 为了最大限度地减少网络的端到端响应时间（End to End Response Time），推理通常比训练批量输入更少的输入样本，也就是更小的批尺寸，因为依赖推理工作的服务（例如，基于云的图像处理管道）需要尽可能的更快响应，因此用户不必要让系统累积图像成更大批尺寸，进而不必等待几秒钟的响应时间。高吞吐量是训练期间最为重要的考量，但低延迟对于推理更加重要。

本章后面展开的内容中，大部分就是围绕以上总结的新问题和挑战展开。

接下来我们通过MMdnn中自带的一个模型在TensorRT的推理过程实例来了解通过程序中使用推理工具和系统进行的模型推理的常见步骤。

```python
# (1) 将框架(本实例中为TensorFlow模型文件)的模型文件转换为推理系统的模型格式(本实例中为UFF格式).
…
uff_model = uff.from_tensorflow(tf_model, OUTPUT_NAMES)
# (2) 部署模型文件到推理系统TensorRT.
…
engine = trt.utils.uff_to_trt_engine(G_LOGGER, uff_model, parser, 1, 1 << 20)
# (3) 读取测试输入. 
…
img = Image.open(path)
…
# (4) 初始化推理引擎上下文(context).
context = engine.create_execution_context()
...
# (5) 将输入数据拷贝到设备(例如，GPU)并执行模型进行推理
… 
bindings = [int(d_input), int(d_output)]
…
cuda.memcpy_htod_async(d_input, img, stream)
# (6) 模型推理
context.enqueue(1, bindings, stream.handle, None)
…
# (7) 最后，获取推理结果，并将结果拷贝到主存
cuda.memcpy_dtoh_async(output, d_output, stream)
…
print("Prediction: ", LABELS[np.argmax(output)])
# 打印结果
>>> Prediction:  n01608432 kite 
```

类似其他的推理系统也是有类似的步骤。也有很多场景下是通过CPU进行推理，这样就不需要进行主存与设备内存之前的数据拷贝。通过以上代码实例我们可以了解主干流程，那么当把模型部署之后，我们可以通过下图观察到常见推理系统的模块，与推理系统交互的系统和推理任务的流水线。

从下图8-1-4，我们可以看到，模型首先经过训练后会保存在文件系统中，随着训练处的模型效果不断提升，可能会产生新版本的模型，并存储在文件系统中。之后模型会通过服务系统部署上线，推理系统会加载模型到内存，同时会对模型进行一定的版本管理，对输入数据进行批尺寸(Batch Size)动态优化，并提供服务接口（例如，HTTP，gRPC等），供客户端调用。

<center> <img src="./img/1/8-1-4-servingsys.png" width="700" height="200" /></center>
<center>图8-1-4. 推理服务系统</center>


## 8.1.2 推理系统的优化目标与约束


我们以在线推荐系统的服务需求为例，去了解应用场景层面对推理系统产生的需求。例如，某在线新闻APP公司希望部署内容个性化推荐服务并期望该服务能满足以下需求：

- 低延迟：
  - 互联网上响应请求延迟常常期望小于100毫秒
- 高吞吐：
  - 突发新闻驱动的暴增人群的吞吐量需求
- 扩展性：
  - 扩展到不断增长的庞大的用户群体
- 准确度：
  - 随着新闻和读者兴趣的变化提供准确的预测 

<center> <img src="./img/1/8-1-5-recommendsys.png" width="230" height="300" /></center>
<center>图8-1-5. 推荐系统</center>

除了应用场景的需求，推理系统也需要应对不同模型训练出的框架和多样性的推理硬件所产生的部署环境多样性，部署优化和维护困难且容易出错的挑战：

<center> <img src="./img/1/8-1-6-flexibility.png" width="600" height="180" /></center>
<center>图8-1-6. 推理系统部署需要支持多种框架和硬件</center>


- 框架多样：
  - 大多数框架都是为训练设计和优化：例如，批尺寸(Batch Size)，批处理作业对延迟不敏感，分布式训练的支持，需要更高的数据精度支持等。	
  - 开发人员需要将必要的软件组件拼凑在一起：需要兼容数据读取，客户端请求，模型推理等多个阶段的处理。
  - 跨多个不断发展的框架集成和推理需求：PyTorch和TensorFlow等框架，以及不同的框架版本的支持。
- 硬件多样：
  - 多种部署硬件的支持：移动端部署场景和约束多样（例如，自动驾驶，智能家居等），芯片厂商多，提供的芯片多样。

综上所述，我们可以总结出设计推理系统的主要的优化目标：

- 低延迟（Low Latency）:
  - 满足服务等级协议的延迟
- 高吞吐量（High Throughputs）:
  - 能够支持高吞吐量的服务请求
- 高效率（High Efficiency）: 
  - 高效率，低功耗使用GPU, CPU
- 灵活性（Flexibility）: 
  - 支持多种框架, 提供构建不同应用的灵活性
- 可靠性（Reliability）:
  - 需要对不一致的数据，软件、用户配置和底层执行环境故障等造成的中断有弹性（Resilient）。
- 可扩展性（Scalability）:
  - 扩展支持不断增长的用户或设备

除了以上的优化目标，设计推理系统时，相比训练系统，需要满足更多的约束：

<center> <img src="./img/1/8-1-7-constraint.png" width="600" height="500" /></center>
<center>图8-1-6. 推理系统部署需要支持多种框架和硬件</center>

- 服务等级协议(SLA)对延迟的约束：例如，某应用希望模型返回请求在100ms以内。
- 资源约束
  - 设备端功耗约束：IOT设备提供的电池电量有限，无法承受计算量过大产生过大能耗的模型部署。
  - 设备与服务端内存约束：移动端设备的内存相比服务端的服务器内存要小很多。
  - 云端资源的预算约束：云端部署应用是资源和使用时长付费，一般公司有指定的预算限制。
  - …
- 准确度(Accuracy)约束
  - 使用近似模型产生的一些误差可以接受：可以通过模型压缩，量化，低精度推理等手段进行推理加速，以精度换速度。

通过下图我们可以看到推理系统的全貌。首先从流程上来看，推理系统完成以下处理并涉及以下系统设计问题:

<center> <img src="./img/1/8-1-8-infersys.png" width="800" height="500" /></center>
<center>图8-1-7. 推理系统组件与架构 <a href="https://developer.nvidia.com/tensorrt">图片引用</a></center> 

如图图8-1-7.所示，推理系统中常常涉及相应模块并完成相应功能，我们将在后面章节中逐步展开：

1. 请求与响应处理：系统需要序列化与反序列化请求，并通过后端高效执行，满足一定的响应延迟
2. 请求调度：系统可以根据后端资源利用率，动态调整批尺寸，模型的资源分配，进而提升资源利用率，吞吐量。
3. 后端框架执行：框架将请求映射到模型作为输入，并在运行时调度深度学习模型的内核进行多阶段的处理。
4. 模型版本管理：在云端算法工程师不断验证和开发新的版本模型，需要有一定的协议保证版本更新与回滚。
5. 健康汇报：云端的服务系统应该是可观测的，才能让服务端工程师监控，报警和修复服务，保证服务的稳定性和SLA。
6. 推理芯片代码编译：在边缘端等场景会面对更多样的硬件，驱动和开发库，需要通过编译器进行一定代码生成让模型可以跨设备高效运行，并通过编译器实现性能优化。

推理系统一般可以部署在云或者边缘。云端部署的推理系统更像传统Web服务，在边缘侧部署的模型更像手机应用和IOT应用系统。两者有以下特点：
1. 云（Cloud）端：云端有更大的算力，内存，且电更加充足满足模型的功耗需求，同时与训练平台连接更加紧密，更容易使用最新版本模型，同时安全和隐私更容易保证。相比边缘侧可以达到更高的推理吞吐量。
2. 边缘（Edge）端：边缘侧设备资源更紧张（例如，手机和IOT设备），且功耗受电池约束，需要更加在意资源的使用和执行的效率。

推理系统中应用了大量的策略与工具进而让模型可以部署在多样的环境中达到更好的性能和满足资源约束。图8-1-8中所示，一般我们可以将引入的策略分为在线和离线策略，离线策略是在模型还没有加载到推理系统内存部署的阶段，对模型本身进行的转换，优化与打包，在线部分是推理系统加载模型后，运行时对请求和模型的执行进行的管理和优化。我们将在后面的章节中介绍相应的策略，让读者对推理系统也形成系统化（Systematic）的感知。

<center> <img src="./img/1/8-1-9-policyinfer.png" width="500" height="300" /></center>
<center>图8-1-8. 推理系统中涉及到的策略与优化</center>

 ## 小结与讨论

本小节主要围绕深度学习推理系统设计需要考虑的多目标和约束展开介绍，希望大家在深入了解推理系统之前，能对推理系统本身有一个更加全面的认识，之后的各种优化策略也是围绕这些需求进行权衡和设计。

最后大家可以进而思考以下问题，巩固之前的内容：
推理系统相比传统服务系统有哪些新的挑战？
云和端的服务系统有何不同的侧重和挑战？

## 参考文献

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
- [GPU-Based Deep Learning Inference: A Performance and Power Analysis](https://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf)
- https://developer.nvidia.com/blog/inference-next-step-gpu-accelerated-deep-learning/